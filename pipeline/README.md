# Gary Machina Labs Data Engineer Homework

# Prerequisits
1. MacOS or other systems capable of running Makefile
1. Docker, tested with Docker Desktop 4.33.0 installed on MacBook Pro with 8GB memory reserved for Docker

# Instructions
1. Clone this github repo and switch to `solution` branch.

 ```bash
 git clone -b solution git@github.com:garyluoex/gary_ml_ops_data_engineer_hw.git
 ```

2. Once finish cloning, change directory into the cloned repo's `pipeline` directory.

```bash
cd gary_ml_ops_data_engineer_hw/pipeline
```

3. [Install Docker](https://docs.docker.com/engine/install/) if you haven't already, and make sure docker is currently running. 

4. Run the following command to build the Airflow docker image and start the Airflow container.
```bash
 make run
```

4. Wait 15 seconds for the airflow webserver inside the container to start.

5. Login to Airflow UI at [http://localhost:8080](http://localhost:8080) using username: `admin`, password: `admin`.

7. To start processing data, enable `preprocess_pipeline` DAG by turning on the toggle to the left of the name `preprocess_pipeline`

8. Wait for `preprocess_pipeline` to complete indicated by a `1` inside a dark green circle in the `Runs` column and `preprocess_pipeline` row.

9. Next lets generate features with `features_pipeline`, enable the `features_pipeline` DAG by turning on the toggle to the left of the name `features_pipeline`.

8. Wait for `features_pipeline` to complete indicated by a `1` inside a dark green circle in the `Runs` column and `features_pipeline` row.

9. Next lets compute run statistics with `summary_pipeline`, enable the `summary_pipeline` DAG by turning on the toggle to the left of the name `summary_pipeline`.

12. Wait for `summary_pipeline` to complete processing.

13. Once all three pipeline completes, the generated data will be stored in the `data/pipeline_artifacts/` folder as [deltalake tables](https://delta-io.github.io/delta-rs/usage/installation/) (a table wrapper around parquet data).

```bash
# Intermediate Tables
data/pipeline_artifacts/preprocess_pipeline/paritioned_data      # raw data paritioned for efficiency
data/pipeline_artifacts/preprocess_pipeline/pivoted_data         # table pivoted into wide format
data/pipeline_artifacts/preprocess_pipeline/interpolated_data    # table with sensor misalign gaps filled using interpolation

# Final Tables
data/pipeline_artifacts/preprocess_pipeline/combined_data        # complete data with all features
data/pipeline_artifacts/preprocess_pipeline/statistics_data      # table with statistics on each run
```

14. To query the deltalake tables shown above, I recommend installing [DuckDB CLI](https://duckdb.org/docs/installation)
    1. Install DuckDB CLI
    2. Make sure you are still in the `pipeline` folder
    4. Find `query_pipeline_artifacts.sql` file inside the `pipeline` folder
    5. You can use the SQL command inside to create SQL tables from deltalake table
    3. Start DuckDB CLI shell by using this command `duckdb`
    6. Start querying the data!

15. Alternatively you can query deltalake tables using pandas dataframe as well by
    1. SSH into the Airflow container using `make ssh`
    2. Start python shell `python`
    3. Follow the instruction here to convert delta lake table to pandas https://delta-io.github.io/delta-rs/usage/querying-delta-tables/

18. To stop the airflow container, use `make stop` command.

17. To clean up artifacts, run `make clean` which will remove the docker image and remove data generated by the pipeline inside `data/pipeline_artifacts`.


# Assumption
1. Sensor data is streamed into storage and there is no gurantee when or in what order the sensor data from a run will be delivered.
2. Assume every sensor data record is delivered excactly once.

# Pipeline Strategy
1. The key insight to the design of this pipeline is that each run of the pipeline only processes a data from runs that are recently updated.
2. And since there is no gurantee when a run is complete, we always process all the sensor data belonging to a run regardless of when the sensor data is ingested even if there is only one new sensor data record ingested.
3. Each task is implemented with idempotentency in mind, so that reprocessing is supported and will not cause duplicate data.
4. Most columns (except `run_uuid` and `time`) allow null values and it is up to the person implementing features to determine what the beahvior of the feature should be when its dependent column's value is null.

# Future Work
1. When doing backfill/reprocessing, a run could get reprocessed multiple times unmecessarily since a run's records can belong to multiple data intervals.
2. Currently the `feature_pipeline` will not wait for `preprocess_pipeline` to complete before proceeeding. This can be solved in production with concurrent processing support.
3. Only manual testing and validation has been done, need to investigate [deltalake table constraint feature ](https://delta-io.github.io/delta-rs/usage/constraints/) and airflow unit test capability.
